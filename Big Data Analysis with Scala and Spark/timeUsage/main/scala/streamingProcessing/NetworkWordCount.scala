package streamingProcessing

/**
  * Created by LENOVO on 2017/3/26.
  */
import org.apache.spark._
import org.apache.spark.streaming._
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.dstream.{DStream, ReceiverInputDStream}
import org.apache.spark.sql.SparkSession
import org.apache.spark.storage.StorageLevel
//reference:
//https://aiyanbo.gitbooks.io/spark-programming-guide-zh-cn/content/spark-streaming/a-quick-example.html
object NetworkWordCount {

  //  lazy val sparkSQL = new SparkSession.Builder().appName("myApp").config("spark-master", "local[2]").getOrCreate()
  //创建了一个， 具有两个执行线程线程一级一秒流处理的本地streamingContext
  def main(args: Array[String]): Unit ={

        if (args.length < 2){
          System.err.println("Usage: NetworkWordCount <hostname> <post>")
          System.exit(1)
        }

    StreamingExample.setStreamingLogLevel()

    lazy val conf = new SparkConf().setMaster("local[2]").setAppName("myStreaming")
    lazy val ssc = new StreamingContext(conf, Seconds(10))
    // Create a socket stream on target ip:port and count the
    // words in input stream of \n delimited text (eg. generated by 'nc')
    // Note that no duplication in storage level only for running locally.
    // Replication necessary in distributed scenario for fault tolerance.
        val lines: ReceiverInputDStream[String] = ssc.socketTextStream(args(0), args(1).toInt, StorageLevel.MEMORY_ONLY_SER)
//    val lines = ssc.socketTextStream("localhost", 1080)
    //？？socketStream有何不同

    val words = lines.flatMap(_.split(" "))
    val wordCounts = words.map(word => (word, 1)).reduceByKey(_+_)
    //将每个rdd存储为文件
    //https://aiyanbo.gitbooks.io/spark-programming-guide-zh-cn/content/spark-streaming/basic-concepts/output-operations-on-DStreams.html
    //saveAsTextFiles(prefix, suffix) 两个参数分别是前缀名和后缀名，前缀名指明地址，后缀名可以利用系统时间，保证文件不同
    wordCounts.saveAsTextFiles("file:///D:/file/port"+args(1), System.currentTimeMillis.toInt.toString)
    ssc.start()
    ssc.awaitTermination()

  }


}


